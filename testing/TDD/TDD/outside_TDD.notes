The Testing Pyramid
Learn about the testing pyramid

Full-stack web applications are often large and complex, and can grow to serve hundreds of different views to millions of users. As the size of a codebase and number of users grow, the cost of bugs compounds — from losing users to slowing production.

You can mitigate the frequency of costly bugs by adding a test suite to your codebase. The purpose of a test suite is to catch errors before you deploy changes to your web application. Without proper attention, however, a test suite can become bloated or ineffective. In this article, you will learn about the testing pyramid, a framework for designing a test suite that optimizes for speed and confidence.

By the end of this article, you will understand an approach to building test suites that optimize for both speed and confidence.

Example
Most full-stack web applications include the following layers:

A view that appears in a user’s web browser
A server that handles HTTP requests
A database that stores information about user interactions
Throughout this article, we will use the following example to illustrate the testing needs in each layer of a full stack web application.

Imagine you are developing a movie blog and want to build a feature that allows users to comment on your posts. Any comment can be read by any other user on the website.

This is how a user may interact with your web application:

The browser renders comments below the blog post.
When a user wants to post a comment, they enter text into a box and click the Submit button.
Your web application transmits the comment to your server.
Your server checks if the POST has any errors. If it does not, it creates a new comment and saves it to the web application’s database.
The next user to arrive will see the last comment at the top of the list of comments.
In the next section, we will begin to learn about the types of tests we can run against the full-stack web application above.

Integration Tests and Unit Tests
We call some web applications “full stack,” because we think of the view, server, and model/database as separate layers — each one sitting on top of and relying on the one below it. This structure impacts the types of tests that we can run against our web application.

Developers think of tests as fitting into a spectrum. The spectrum represents the amount of the web application that a test exercises.

￼
Let’s consider some of the vocabularies in the spectrum above. Unit tests

Unit tests are isolated and fast tests that check one small behavior within your web application.

For example, we want to test whether our database can save a comment. Saving does not involve the view or server. We can create a test that writes directly to the database, and the test itself doesn’t need to know about the other layers.

A test like this is fast, but only gives you confidence that a small slice of your system is working as intended.

System tests System tests are a group of fully integrated tests that exercise your entire web application.

For example, we want to test whether our blog renders with the correct post and comments. We can write a test that checks whether a browser renders a stored blog post. This test exercises every layer of the web application:

The database stores the blog post.
The server sends the HTML to the browser.
The browser renders the view.
A test like this is slow and less descriptive but provides you with confidence that a large slice of your system is working as intended. Integration tests Integration tests include everything between unit tests and system tests. They exercise multiple parts of the web application, often in different layers.

For example, an integration test may check whether your web application can save a server-generated comment to the database. This test integrates two layers of your web application:

The server receives the comment and sends it to the database.
The database stores your comment.
Developers often call tests like the one above end-to-end tests, because they start in the browser (one end) and traverse the stack to the database (other end).

Integration tests are faster than system tests, but slower than unit tests. They provide less confidence (that your feature works as expected) than system tests and more confidence than unit tests.

Shape of a testing suite
The goal of a full-stack web application’s testing suite is to provide you with confidence that your application works as expected while executing in a timely fashion.

How would you use integration and unit tests to accomplish this?

You could write few integration tests that provide you with confidence, and more unit tests that execute quickly and provide you with specific feedback.
The number and types of unit and integration tests that you write can be mapped onto the testing pyramid.

The Testing Pyramid
The testing pyramid is an approach to structuring your test suite.

Browser-level integration tests sit on the top of the pyramid. This layer is the narrowest because it should have the fewest number of tests — the slow nature of browser-level tests make them more expensive than server-level tests.

While server tests don’t need to interact with the browser, they usually exercise parts of the model or database. They sit close to the middle of the spectrum between unit and system tests. They provide a moderate level of confidence as they may exercise multiple layers of the stack. Server tests are more expensive and provide less specific error messages than model tests.

Compared to browser and server tests, model and database tests exercise a smaller portion of the stack. They provide the most specific feedback, but not much confidence that the whole system is working as expected. Because they are the cheapest, you can write a lot of them without significantly slowing the amount of time it takes to run your test suite.

￼
Example Testing Suite
Most sites that support user commenting set a limit on the number of comments that load when you arrive on the page. If an article has 300 comments, rendering the entire list would take too long and make the web page difficult to navigate.

Imagine that your web application already has a feature-level integration test that checks if the browser renders comments to the site.

Let’s consider the most efficient set of tests we could write to check that the browser renders only the last ten comments.

Because you already have a test at the feature layer, you decide to start by writing a server test. The test will check if:

Calling a Comments.latest method returns a list of comments from your database that is ten items or less.
The server and feature level tests provide you with confidence that your web application returns comments that don’t exceed the maximum length. However, they don’t provide any information about which comments the model layer returns.

Once, you have the confidence that your model layer returns ten comments or less from your database, you can check the specifics of the Comments.latest method. You can check if:

A database with more than ten comments returns only ten.
A database with less than ten comments returns all of the comments.
A database with zero comments returns zero comments.
The list of comments is in reverse chronological order.
To support a ten-comment limit, we added one server test and four model tests. Notice, our tests provide greater detail as we get closer to the database. The tradeoff is that each test at the model layer provides us with less confidence that the system is working as expected.

Conclusion
While the testing pyramid is an approach to optimize your test suite, it’s important to think critically about the tests you write at each layer.

When making decisions about how to test a feature, you should ask yourself a few of these questions:

Is a feature-level integration test necessary?
Can I test the same behavior with server and model layer tests?
How much confidence do I have with the server and model layer tests?
How long does the feature test take? Will that impact my team’s workflow?
Outside-In Test-Driven Development
Learn outside-in TDD

When software engineers and development teams build new features, they’re faced with challenging questions along the way. Some include:

Where do we start development? (Do I start writing HTML, or adding server implementation?)
What is the most efficient implementation? (Is X implementation faster than Y implementation?)
How will the new feature impact our existing code? (Will new code introduce a bug?)
Outside-in test-driven development doesn’t answer these questions, but provides you with an approach to finding an answer. It helps you avoid the decision paralysis that often slows development teams, leads to extended deadlines, and incomplete implementation.

In addition to the efficiencies discussed above, the outside-in approach is a satisfying way to develop a web application with a full test suite.

At the end of this article, you will know how to approach development of features in a full-stack web application using outside-in test-driven development.

Red, Green, Refactor
Test-driven development (TDD) is the process of writing tests before implementation code. You use the feedback from your tests to inform the implementation of a feature or outcome.

A common approach to TDD is the red, green, refactor cycle. When you write a test before the implementation exists you start “in the red” phase, because your test fails and outputs a red error message. Next, you write the minimum implementation code to get your test to pass. This puts you “in the green” phase, because your test passes and outputs a green message.

Once you are in the green, you should consider whether your implementation is the best or most efficient approach. If you think your code could be written more efficiently or cleaner, then you enter the refactor phase. You can refactor your code with confidence, because you have tests that cover the expected behavior.

Outside-in TDD
Outside-in TDD is an approach that developers use to build full-stack web applications. It leverages the same red, green, refactor steps that we covered above, but with one caveat — a failing test does not always inform you to write new implementation code. Instead, it may require that you implement new functionality at a different level.

You start at the top of the stack, the view, and write tests as you work your way towards the database layer.

If a test pushes you to a lower level, you restart your red, green, refactor cycle by writing a new test. This test informs the implementation at your new layer. You continue the TDD cycle at this lower level until:

You need to drop another layer to implement the desired behavior
You have addressed the reason for dropping to the current layer
Once you address the reason for dropping a layer, you can start working your way back up the testing pyramid. If you’re in the model/database layer, you step up to the server, and run your server tests to see if you get a different response. The response should be one of the following:

The test passes — you can start another red, green, refactor cycle at the server level or step up to the view layer.
The test fails — the server test that pushed you to the model layer fails, but for a different reason. This is common, and indicates that you’re making progress. This failure may indicate that you need to write additional implementation at the server level, or drop back to the model.
Outside-in Example
We’re going to use the following as an example of how to develop a new feature with outside-in TDD: You have a movie blog and want to develop a feature that renders user comments under your blog posts. The application should render no more than ten comments when a user lands on the web page. The application should present the comments in reverse chronological order (i.e. the most recent comment should be first).

Let’s assume the web application generates HTML at the server — any updates to the view require implementation at the server level.

Feature Testing
The first step is to write a feature test that checks if your web application is rendering comments to the browser. Let’s use the following outside-in TDD approach:

Write a test that checks for the presence of a comment under a blog post.
The test fails, because your web application does not render comments.
Because your web application generates HTML at the server layer, you drop to the server to address the error.
Although we could continue to write feature tests to check for the number of rendered comments, we know server tests are cheaper, so we can test those details when we drop a layer.

Server Testing
At the server layer, we start by writing a test that informs the implementation of our server-generated HTML. Because our web application renders unique comments from the database, we want to check that the server-generated HTML is dynamic.

Write a test that checks for the presence of a dynamically generated comment element in the server HTML.
The test fails, so we add implementation for a server-generated comment.
Once we’re in the green and consider refactoring, we want to write a test that calls a method at the model layer, let’s call it Comment.latest(). At the server layer, we’ll check if the method returns comments from the database.
Because this method doesn’t exist, we must drop to the model/database layer.
Model and Database Testing
At the model layer, we start by writing a test that informs the implementation of our Comment.latest method. This method requires that you interface with the web application’s database.

Write a test that checks if the Comment.latest method returns ten comments when the database has eleven comments.
Implement the Comment.latest method to return ten comments, so the test is green.
Once you’ve considered refactoring, write a test that checks whether the method returns the last ten comments in reverse chronological order.
Implement and refactor
Write a test that checks if Comment.latest() returns an empty array when your database is empty.
Implement and refactor
Write a test that checks if Comment.latest returns the correct number and order of comments when your database has between zero and ten comments in it.
Implement and refactor
Taking Stock
At this point, your entire test suite should be green. You have written seven new tests, and the implementation code to make them pass — your web application should render the last ten comments from your database in reverse chronological order.

Let’s take stock of our seven new tests:

Feature: Comments are rendered to a user’s browser.
Server: The server generates an HTML field for comments.
Server: The server has access to ten comments from the database.
Model: The Comment.latest method returns ten comments from your database.
Model: The Comment.latest method returns the last ten comments in your database in reverse chronological order.
Model: The Comment.latest method returns an empty array when your database has zero comments.
Model: The Comment.latest method returns all of the comments when your database has between zero and ten comments.
Once your feature is working as expected, you should consider how your new tests fit into the broader test suite. The rest of the test suite could have few tests, or over one hundred. It’s time to refactor.

Refactoring Your Test Suite
The way you approach refactoring will vary based on the size and types of tests in your suite. One guiding light in refactoring is to optimize the suite for confidence and speed. Because we used TDD to implement our comment feature, we should feel confident that our comments are working as expected, and the feature is fully covered.

Consider the questions below when deciding how to refactor your suite:

How much longer does it take to run my test suite with these new tests?
Is the additional amount of time that your test suite takes to run acceptable?
Is there overlap between any of my new tests?
Is there overlap between my new tests and existing tests?
Let’s take a moment to consider a few of these questions in the context of our test suite.

How much longer does it take to run my test suite with these new tests?

You can calculate this value by running your test suite before and after writing the new tests, and calculate the difference. Seven new tests, like the ones above may only add a few seconds to your suite. Let’s use our next question to think about how you can evaluate what an acceptable amount of time may be.
Is the additional amount of time that your test suite takes to run acceptable?

Although a few seconds may seem acceptable, this time can add up as your suite grows. Even if you’re comfortable with the additional time, you should always consider whether you can make speed improvements that don’t impact confidence.
Is there overlap between any of my new tests?

You should consider if any new tests, especially in the feature or server level, can be deleted without impacting your confidence that the comments feature works as expected. For example, our first server test checks if the server generates an HTML field for comments. Your feature-level test checks the same functionality — it also takes longer, but provides a higher level of confidence. We decide to delete the server test for reasons we will investigate when we consider our next question.
Is there overlap between my new tests and existing tests?

Next, you should look outside your seven (now six) new tests to consider the coverage offered by the other tests in your suite. Often, your test suite will have a feature test that checks whether the web page renders as expected — this is usually good enough coverage for most new features. Given the cost of feature-level tests, and the coverage of your lower-level tests, it often makes sense to delete the new feature-level test.
